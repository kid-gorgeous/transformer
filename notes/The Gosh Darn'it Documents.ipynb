{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b24a66c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ScaledDotProductAttention Model\n",
    "\n",
    "--- \n",
    "\n",
    "#### MultiHeadAttention Model\n",
    "\n",
    "Containing the super call, this will extend the nn.Module. The MultiHead Attention mechanism computes the attention beween each pair of positions in a sequence. It consists of multiple \"attention heads\" that capture different aspects of the input sequence.\n",
    "\n",
    "The code will initialize the module with input parameterize and linearize the transformation layers. It calculates attention scores, reshapes the input tensor, and splits it into multiple heads, and combines the attention outputs from all heads. The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence. \n",
    "\n",
    "#### Position-wise Feed-Forward Networks\n",
    "\n",
    "The PositionWiseFeedForward class extends PyTorch's NN Modules and implements a postion-wise feed-forward network. The class initializes with two linear transformation layers and a ReLU activation function. The forward method applies these transormations and activation function sequentially to compute the output. This process enables the doel to consider the position of input elements while making predictions.\n",
    "\n",
    "#### Postional Encoding\n",
    "\n",
    "Postional Encoding is used to inject the postion information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.\n",
    "\n",
    "The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values. The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor d0v_term. The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the postion information of the input sequence.\n",
    "\n",
    "#### Encoding Layer\n",
    "\n",
    "This consists of MultiHead Attention Layers, a PositionWise FeedForward layer, and two Layer Normalization layers.\n",
    "\n",
    "The encoder layer class initailizes with input parameters and components, including a MultiHeadAttention modules, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. It will then compute the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.\n",
    "\n",
    "#### Decoding Layer\n",
    "\n",
    "The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention, a PostionWiseFeedForward module, three layer normalization modeules, and a dropout layer. \n",
    "\n",
    "The forward method computes the decoder layer output by performing the following steps:\n",
    "\n",
    "1. Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization\n",
    "\n",
    "2. Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized masked self-attention output, followed by dropout and layer normalization.\n",
    "\n",
    "3. Calculate the position-wise feed-forward output and combine it with the mormalized cross-attention output, followed by dropout and layer normalization. \n",
    "\n",
    "4. Return the processed tensor. . .\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "The transformer class combines the preciously defined modules to create a complete Transformer model. During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer.\n",
    "\n",
    "The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens. The forward method computes the Transformer model's output through th following steps: \n",
    "\n",
    "1. Generate source and target masks using the generate_mask method\n",
    "2. Compute source and target embeddings, and apply positional encoding and dropout\n",
    "3. Process the source sequence through encoder layers, updating the enc_output tensor.\n",
    "4. Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.\n",
    "5. Apply the linear projection layer to the decoder output, obtaining output logits.\n",
    "\n",
    "\n",
    "#### Preparing Sample Data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
